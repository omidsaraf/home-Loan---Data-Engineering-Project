## 🧪 Data Validation & Quality Testing Framework

This module ensures rigorous validation of your HomeLoanIQ data pipeline, which handles Bronze, Silver, and Gold layers using PySpark and metadata-driven logic. It covers both batch and streaming scenarios, enforces data quality (DQ) policies, and integrates seamlessly with CI/CD (via GitHub Actions or Azure DevOps).

---

## 📌 Objectives

* Validate schema and structural consistency across all layers
* Detect null anomalies, duplication, partition imbalance, and referential issues
* Enforce data contracts and expectations from metadata tables
* Enable test-driven development and continuous validation for every notebook

---

## 🛠️ Technologies Used

| Tool                      | Purpose                                                     |
| ------------------------- | ----------------------------------------------------------- |
| `PyTest`                  | Unit testing for PySpark logic and API endpoints            |
| `great_expectations`      | Declarative DQ rules and expectation suite validations      |
| `Spark Session Mocking`   | Mocks for testing transformations in local/dev environments |
| `Delta Expectations`      | Delta table-level constraints (not null, unique, etc.)      |
| `GitHub Actions / DevOps` | CI/CD pipeline test execution and feedback                  |
| `Databricks Repos`        | Source control for versioned notebooks and test logic       |

---

## 📁 Directory Structure

```bash
/tests
├── unit/
│   ├── test_bronze_stream_schema.py
│   ├── test_batch_ingestion_logic.py
│   ├── test_kpi_calculations.py
│   └── test_influence_graph_generation.py
├── integration/
│   ├── test_metadata_driven_run.py
│   ├── test_streaming_trigger.py
│   └── test_batch_pipeline_sequence.py
├── dq/
│   ├── expectations/                      # Great Expectations JSON/YAML rules
│   └── rules/
│       ├── null_thresholds.json
│       ├── unique_key_constraints.json
│       └── referential_integrity.yaml
├── configs/
│   └── pytest.ini
└── resources/
    └── synthetic_inputs/                 # CSV/Parquet for mocking input datasets
```

---

## ✅ Unit Tests

Each PySpark transformation function is unit-tested with:

* Schema validation
* Transformation correctness
* Edge-case handling using parameterized inputs

```bash
pytest tests/unit/
```

✅ Example:

```python
def test_customer_stage_transformation(spark_session):
    df = spark_session.read.csv("resources/synthetic_inputs/customer_stage.csv", header=True)
    result = apply_stage_transformation(df)
    assert result.columns == expected_columns
    assert result.count() > 0
```

---

## 🔁 Integration Tests

Simulate full data flow pipelines using mocked metadata + synthetic batch/stream input files.

```bash
pytest tests/integration/
```

🧪 Example:

* Validate metadata ingestion triggers the correct notebook
* Ensure gold-layer output matches expected structure
* Confirm partitioning and merge key consistency

---

## 📏 Data Quality Validation

**Great Expectations** is used to apply declarative DQ rules:

### ✅ Sample Checkpoints:

```bash
great_expectations checkpoint run test_bronze_ingestion
```

### Rules Include:

| Check Type            | Description                                  |
| --------------------- | -------------------------------------------- |
| **Null Threshold**    | Detect excessive nulls on required fields    |
| **Duplicate Key**     | Check for uniqueness of natural/business PKs |
| **Referential Links** | Ensure IDs exist in dimension tables         |
| **KPI Tolerances**    | Validate Gold KPI shifts within 5% threshold |

---

## 🔐 Observability & Security

* Test results are saved to `/mnt/quality_logs/` and optionally pushed to Azure Monitor
* Any test failures are flagged to Slack/email via GitHub Actions hooks
* Secrets used in testing (e.g., storage keys) are pulled from Key Vault or `.env` via `python-dotenv`

---

## ⚙️ CI/CD Integration Example (GitHub Actions)

```yaml
jobs:
  test_pipeline:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install Dependencies
      run: pip install -r requirements.txt

    - name: Run PyTest Unit Tests
      run: pytest tests/unit/

    - name: Run Integration Tests
      run: pytest tests/integration/
```

---

## 📊 Reporting

| Layer       | Validation Scope                                  | Tools Used             |
| ----------- | ------------------------------------------------- | ---------------------- |
| Bronze      | Schema checks, record count, ingestion integrity  | PyTest + GE            |
| Silver      | Join conformance, enrichment logic                | PyTest + Spark Asserts |
| Gold        | KPI business logic, window functions, PII masking | PyTest + Expectations  |
| Graph Table | Edge/node integrity, rank logic, influence rules  | PyTest + PyGraph       |

---

## ✅ Testing Strategy Summary

| Layer     | Test Types                       | Blocking |
| --------- | -------------------------------- | -------- |
| Bronze    | Schema, nulls, duplicates        | ✅ Yes    |
| Silver    | Transformations, enrichment      | ✅ Yes    |
| Gold      | KPI validation, accuracy         | ✅ Yes    |
| API       | Endpoint response, token auth    | ✅ Yes    |
| Streaming | Triggering, checkpoint integrity | ✅ Yes    |
| Graph     | Relationship validation, rank    | ✅ Yes    |


