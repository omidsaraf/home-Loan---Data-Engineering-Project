
This directory contains the **core data processing logic** for the Home Loan Intelligence platform, following a **Bronze â†’ Silver â†’ Gold** architecture.

---

### ğŸ“ Folder Structure

```plaintext
src/
â”œâ”€â”€ ingestion/          # Raw file ingestion logic (batch/API/stream)
â”œâ”€â”€ transformation/     # ETL scripts categorized into Bronze/Silver/Gold
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ extraction/         # Export scripts for Tableau, APIs, Salesforce
â”œâ”€â”€ notebooks/          # Exploratory notebooks (Databricks/Zeppelin)
â””â”€â”€ utils/              # Reusable functions, validations, config loaders
```

---

## ğŸ”„ 1. `ingestion/` â€“ Data Ingestion Pipelines

Handles ingestion from:

* Core banking (via ADF or API)
* CRM / Loan Origination System (REST, JSON)
* Bureau Data (CSV/Excel, SFTP)
* Real-time Kafka streams

**Key Files:**

| File                              | Purpose                                |
| --------------------------------- | -------------------------------------- |
| `ingest_crm_applications.py`      | Load CRM application data              |
| `ingest_corebank_loanbook.py`     | Load core banking disbursal data       |
| `ingest_credit_bureau_batch.py`   | Batch load credit scores               |
| `ingest_stream_kafka_realtime.py` | Consume Kafka topic for real-time apps |

All ingestion jobs follow this **pattern**:

```python
def run_ingestion(metadata: dict):
    df = read_source(metadata)
    df_clean = clean_and_standardize(df)
    write_to_delta(df_clean, metadata['target_path'])
```

---

## ğŸ” 2. `transformation/` â€“ ETL Pipeline Logic

Organized into **Bronze**, **Silver**, and **Gold** layers:

### ğŸªµ `bronze/`: Raw Ingested Staging

* Minimal processing (deduplication, timestamp formatting)
* Schema alignment using JSON contracts
* Saved as Delta Tables partitioned by date

Example: `bronze_crm_applications.py`

```python
df = spark.read.format("delta").load(bronze_path)
df = deduplicate(df, ["ApplicationID"])
df.write.mode("overwrite").saveAsTable("bronze.crm_applications")
```

---

### âœ¨ `silver/`: Cleansed, Enriched Data

* Joins across sources (CRM â†” CoreBank â†” Bureau)
* Adds derived columns: `LoanToValue`, `IsHighRisk`, etc.
* Standardizes timestamp, customer keys, channel types

Example: `silver_enriched_loans.py`

```python
df = join_crm_core(df_crm, df_core)
df = enrich_with_bureau_score(df, df_bureau)
df.write.format("delta").mode("overwrite").saveAsTable("silver.enriched_loans")
```

---

### ğŸ… `gold/`: KPI and Reporting Models

* Pre-aggregated KPI tables for dashboarding
* Modular SQL templates (Net Disbursal, Approval Rate, TAT)
* Filters and dimensions applied (Region, Branch, Product)

Example: `gold_kpi_disbursal_rate.py`

```python
df = calculate_disbursal_kpi(df_enriched)
df.write.mode("overwrite").saveAsTable("gold.kpi_disbursal_rate")
```

---

## ğŸ“¤ 3. `extraction/` â€“ Data Serving & APIs

* Exports data to:

  * Tableau extracts (via Hyper API)
  * Salesforce updates
  * External APIs

**Examples:**

* `extract_tableau_loan_kpi.py`
* `push_to_salesforce.py`
* `api_borrower_profile.py`

---

## ğŸ““ 4. `notebooks/` â€“ Analysis & Debugging

Contains Databricks/Zeppelin notebooks for:

* Profiling new sources
* Data quality explorations
* KPI validation and stakeholder walkthroughs

Naming pattern: `explore_<source>.ipynb`

---

## ğŸ§° 5. `utils/` â€“ Shared Functions

Reusable modules for:

* Spark validation logic (e.g., null checks, schema match)
* Great Expectations integration
* Metadata reading
* Logging and alerting

**Key files:**

* `validator.py`
* `metadata_reader.py`
* `alert_slack.py`
* `spark_utils.py`

---

## âœ… Best Practices

| Area                     | Practice                                                   |
| ------------------------ | ---------------------------------------------------------- |
| File Naming              | `verb_source_context.py` (`ingest_crm_applications.py`)    |
| Function Reuse           | Move generic logic to `utils/`                             |
| Parameterization         | Metadata-driven (via `job.csv`, `proc_param.csv`)          |
| Delta Lake Usage         | Bronze â†’ Silver â†’ Gold format                              |
| Data Contract Compliance | Schema validation via JSON or Great Expectations           |
| Modularity               | Break up large flows by domain (Customer, Loans, Products) |


